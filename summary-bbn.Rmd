---
title: "Summary of Bayesian Beliefs Networks"
author: "Jong Kim"
date: "2024-08-13"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Some useful pointers

## A tutorial

Krieg, M. L. (2001). A tutorial on Bayesian belief networks (No. DSTO–TN–0403). Edinburgh, South Australia, Australia.


# Using a R package, bnlearn

Let's test an example by Jacinto Arias, <https://jacintoarias.github.io/bayesnetRtutorial/#>

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}
library(tidyverse)
library(lubridate)
library(bnlearn)

```

## Data Structures

The usage of bnlearn revolves around the usage of two main **data structures** to represent a Bayesian Network in different stages (NOTE that these are *S3* classes and the names might overlap with other functions):

* `bn` [[Docs]](http://www.bnlearn.com/documentation/man/bn.class.html). Represents the structural information, variables, graph and learning algorithm if provided.
* `bn.fit` [[Docs]](http://www.bnlearn.com/documentation/man/bn.fit.class.html). Adds the parametric information on top of the previous structure. Contains the distribution of each node according to its type and parent configuration.


## Creating the structure of Bayesian networks

There are different ways to manually initialize and modify the graph of a Bayesian Network.

We can create empty graphs from a set of variables:

```{r}
vars <- LETTERS[1:6]
dag  <- empty.graph(vars)

dag
```

You can specify arcs sets in various ways. The most common are:

As a two column (`from`, `to`) matrix, setting them via `arcs` to an existing network:

```{r}
e <- matrix(
      c("A", "C", "B", "F", "C", "F"),
      ncol = 2, byrow = TRUE,
      dimnames = list(NULL, c("from", "to"))
    )

arcs(dag) <- e

dag
```

We can also use an adjancecy matrix, and assign it to a dag with `amat`:

```{r}
adj <- matrix(
        0L, 
        ncol = 6, 
        nrow = 6,
        dimnames = list(vars, vars)
       )

adj["A", "C"] = 1L
adj["B", "F"] = 1L
adj["C", "F"] = 1L
adj["D", "E"] = 1L
adj["A", "E"] = 1L
print(adj)

amat(dag) <- adj

dag
```

The last option is to create a *formula* for a given set of variables. Each node is specified by a pair of brackets `[<var_name>]`. If the node has a parentset we denote it by `|` and specify the list of parents separated by colons `:`. We can compute the formula into a `bn` object with `model2network`.

```{r}
dag <- model2network("[A][C][B|A][D|C][F|A:B:C][E|F]")
dag
```

The package provide other capabilities such as **random graphs** and **graph sampling**. In addition there is a range of utilities to manipulate the graphs [[Link]](http://www.bnlearn.com/documentation/man/mb.html) [[Link]](http://www.bnlearn.com/documentation/man/graph.html).


## Plotting graphs

We can ploting graphs using the built in R engine by using `plot` for the `bn` class:

```{r}
plot(dag)
```

Minimal aspects of the plot can be customized as documented in the corresponding help page. Other packages can be used indrectly to plot graphs, `bnlearn` provides connections with some of them but be aware that some of them might be outdated.

Fortunatelly, graphs are a common data structure and we can find lots of utilities to work with them. One of the most common visualization tools for graphs, and for data in general, is the D3 library from the *Javascript* domain. Thanks to the integration of **R shiny** with web technologies we can find wonderful ports such as the `networkD3` package.

The next snippet is just a custom function to transform a `bn` object to the required information to plot a *D3 force graph*.

*Please install* the `networkD3`, be carefull as this package is currently under beta development and future version could break this code.

```{r}
plotD3bn <- function(bn) {
  varNames <- nodes(bn)

  # Nodes should be zero indexed!
  links <- data.frame(arcs(bn)) %>%
    mutate(from = match(from, varNames)-1, to = match(to, varNames)-1, value = 1)
  
  nodes <- data.frame(name = varNames) %>%
    mutate(group = 1, size = 30)
  
  networkD3::forceNetwork(
    Links = links,  
    Nodes = nodes,
    Source = "from",
    Target = "to",
    Value = "value",
    NodeID = "name",
    Group = "group",
    fontSize = 20,
    zoom = TRUE,
    arrows = TRUE,
    bounded = TRUE,
    opacityNoHover = 1
  )
}
```


We can now plot our last generated BN. If the result looks too small you can zoom in and out using your *mouse wheel*.


```{r}
plotD3bn(dag)
```


## Loading Bayesian networks from files

There are different file formats to represent a Bayesian network. They have originated over the years as an effort to create standards or as part of particular propietary systems. Despite the effort there is not a consensuated closed standard, perhaps the closest one is the `xbif` format [[Link]](http://www.cs.cmu.edu/~fgcozman/Research/InterchangeFormat/).

`bnlearn` provides several ways to load BNs from different formats [[Docs]](http://www.bnlearn.com/documentation/man/foreign.html). You should use them when importing and exporting to other systems or to share them with other people, if you plan to use just bnlearn you could just save an `rda` file.


## The Bayes net repository

The maintainers of `bnlearn` also provide a modern R-focused repository for a series of popular Bayesian networks that have been used extensivelly for benchmarking on the literature.

http://www.bnlearn.com/bnrepository/

In here you can find networks with different properties that can be use to test algorithms or explore this or other BN packages. We will now work with the so popular `asia` network: *Asia* is for BNs what *"iris"* is for statistics.

```{r}
# This downloads the RData file from the repository and loads it.
# The bn is loaded into a bn.fit variable called bn
load(url("http://www.bnlearn.com/bnrepository/asia/asia.rda"))
asia <- bn
```


## Discrete networks

We will know explore the contents of this variable. As you can see, this not only contains the structure but the parameters as well, but first we will take a look at the structure:

```{r}
bn.net(asia)
```

```{r}
plotD3bn(asia)
```

Now is the time to review the parameters, this prints each node and the asociated probability table. In this case all variables are **discrete** so the tables would be conditional probability tables.

```{r}
asia
```

We can access individual nodes of the net as in a data.frame:

```{r}
asia$smoke
```

There is also a function to plot the distributions of discrete networks:

```{r}
bn.fit.barchart(asia$smoke)
```

```{r}
bn.fit.barchart(asia$dysp)
```


## Introducing expert knowledge

We can alter the probability tables of a BN, this is useful to override parameters learnt from data or not observed variables. This method allows us to include expert information from the domain of the problem modelled.

To modify a **conditional probability table** you can just directly replace the existing table in the model by extracting it with `coef`. Be careful to maintain the inherent restrictions of the probability distribution.

```{r}
cpt <- coef(asia$smoke)
cpt[] <- c(0.2, 0.8)
asia$smoke <- cpt
asia$smoke
```


## Sampling data from a Bayesian network

`bnlearn` introduces an *R like* function to sample data from a given fitted model `rbn`. We will now sample a dataset from *asia* to test learning from data.

```{r}
# Note that the order of the parameters is inverted from the R sample functions
sampleAsia <- rbn(x = asia, n = 10000)
head(sampleAsia)
```


## Parametric learning from data

We can induce the parameters of a Bayesian Network from observed data. `bnlearn` provides different algorithms for that, while Maximum Likelihood Estimation (MLE) is the most common one.

We can invoke the learning algorithm by using the function `bn.fit`. For that we need a DAG and a compatible dataset:

```{r}
net <- bn.net(asia)
asiaInduced <- bn.fit(x = net, data = sampleAsia)
```

We can now compare the two networks, there should be some discrepacies in the induced one. Notice that extremelly marginal probabilities will not be simulated and thus will not have a significant present in the sample.

```{r}
asiaInduced
```

## Structural learning

In many ocasions the structure of the model is designed by hand. We usually know the variables of the problems and the different causal patterns from expert knowledge, this descriptive information can be tranfered to the model. This is what we call an open box model and provides a powerful framework for many problems, specially when compared to other models that do not provide a clear interpretation of their parameters.

However, there are many situations in which we would like to automatize the structural learning of a model such as causal patterns discovery or a lack of knowledge of the domain.

In other cases we just want to learn about particular dependency relationships between the variables or select the best structure among a particular set.

`bnlearn` specializes in structural learning, and for this reason it provides a range of algorithms to learn the models. There is a complex taxonomy of such algorithms related to the statistical tests, metrics and heuristics in which they are based. Exact learning is a NP-hard problem and thus several approaches have been proposed.

We will briefly study the `hc` algorithm to learn a full structure and the `BIC` score metric to measure the fit of a particular network with a dataset.

The `hc` algorithm can be simply run from a sample of data, although many options can be set to optimize it for a particular problem.

```{r}
networkInduced <- hc(x = sampleAsia)
networkInduced
```

Lets compare it with the original network **golden model** as the algorithm may have introduced some differences given that we used a small data sample.

```{r}
modelstring(asia)
```

We can also compute some metrics to compare the network. The structural Hamming distance determines the amount of discrepancy between the two graphs.

```{r}
shd(bn.net(asia), networkInduced)
```

The BIC metric measures the fit of the structure for a given sample, it also penalizes the number of parameters to avoid overfitting. In this case the result is almost the same, the lower the metric the better, so it seems that the induced model could be biased towards the sample and marginally outperforms the golden model.

```{r}
print(BIC(object = asia, data = sampleAsia))
print(BIC(object = networkInduced, data = sampleAsia))
```


## Gaussian networks

Gaussian networks differ in the class of probability tables that represent them. If all nodes are gaussian we will find Gaussian nodes and conditional Gaussian nodes. Gaussian nodes are encoded by the normal distribution parameters (mean and sd), conditional gaussian are represented as linear reggresion with a coef for each parent, an intercept term and standard deviation of the residuals.

`bnlearn` has some sample gaussian data to test these BNs:

```{r}
data(gaussian.test)
dag = model2network("[A][B][E][G][C|A:B][D|B][F|A:D:E:G]")
model <- bn.fit(dag, gaussian.test)
model
```

To modify a gaussian network node we proceed as in the discrete caser:

```{r}
model$A <- list(coef = c("(Intercept)" = 10), sd = 0)
model$A

```


## Hybrid networks

An hybrid network contains both discrete and continuous variables. **There is a framework restriction in which a discrete variable cannot have any continuous parent**. The parameters are modelled as in the previous cases.

A continuous variable with discrete parents is represented by a conditional Gaussian distribution, where there is a linear gaussian distribution (according to any continuous parents) for each configuration of the discrete parents.

In the next example we will use `custom.fit` to manually load the parameters into a graph:

```{r}
net <- model2network("[A][B][C|A:B]")

cptA  <- matrix(c(0.4, 0.6), ncol = 2, dimnames = list(NULL, c("LOW", "HIGH")))
distB <- list(coef = c("(Intercept)" = 1), sd = 1.5)
distC <- list(
  coef = 
    matrix(
      c(1.2, 2.3, 3.4, 4.5), 
      ncol = 2,
      dimnames = list(c("(Intercept)", "B"), NULL)
    ),
    sd = c(0.3, 0.6)
)

model = custom.fit(net, dist = list(A = cptA, B = distB, C = distC))
model
```

## Inference and probability queries

The inference engine of `bnlearn` is limited, but it can be used to test the networks and to perform basic operations with the models.

By using `cpquery` we can ask for the probability of an **event** given a set of evidence, both of them are boolean expressions involving the variables in the model. We may ask for a particular combination of configurations in the BN and a set of observed statuses for the variables in the evidence.

For example we could ask,  *what is the posibility of a positive cancer diagnosis for a person that smokes?*, in the asia network.

(For cpquery I recommend the most powerfull `lw` algorithm)

```{r}
# First we should observe the prior probability to compare
# TRUE is for empty evidence
cpquery(asia, event = lung == "yes", evidence = TRUE)
```
```{r}
cpquery(asia, event = lung == "yes", evidence = list(smoke = "yes"), method = "lw", n = 100000)
```

Notice that the method is not much stable, so it is better to aggregate a repeated sample of predictions:

```{r}
query <- cpquery(asia, event = lung == "yes", evidence = TRUE)
print(mean(rep(query, 1000)))

query <- cpquery(asia, event = lung == "yes", evidence = list(smoke = "yes"), method = "lw", n = 100000)
print(mean(rep(query, 1000)))
```

The other option is to use `cpdist` to sample cases from the network for a set of nodes in the presence of some evidence, the usage is the same, and we can obtain more stability by increasing the size of the sample.

```{r}
s <- cpdist(asia, nodes = c("lung"), evidence = TRUE, n=1000)
head(s)
```

We can even compute or plot of the distribution:

```{r}
summary(s)
```

```{r}
prop.table(table(s))
```


```{r}
ggplot(s, aes(x=lung)) + geom_bar()
```

---

# Modeling with Bayesian Networks

Now that we have a nice overview on the operation with `bnlearn` we will switch to an applied problem in which we want to build a model from a real context.

## The problem

We are part of a Data Science team from an IoT company that has developed new *Temperature* sensor to monitor data center rooms in order to detect and predict early refrigeration failures. The sensors were installed a year ago, and we need to create a model that would be able to detect specific heat outbreaks or global heating problems in the room.

We are provided with a dataset consisting on a number of measures per day during the previous year for a total of *3 sensors* that have been places in strategic places across room.

We will start to work on the first version of the problem `heatAlarm-lvl1.csv`.

```{r}
dataRaw <- read_csv("./data/heatAlarm-lvl1.csv")
head(dataRaw)
```

## Exploratory analisys

We will start by studying the properties of our problem at hand. First we should look into the distribution of the dataset.

```{r}
summary(dataRaw)
```

We see that we have a complete year of measures and different ranges for each sensor. Now it is interesting to see the completeness of the data, we would also like to se its distribution along dates.

```{r}
dataRaw %>%
  group_by(Date) %>%
  summarise(Measures = n()) %>%
  ggplot(aes(x=Measures)) + geom_histogram()
```

We confirm that the number of measures is stable during the day. We would now observe the distribution of the Temperature measures for each sensor.

```{r}
dataRaw %>%
  gather("Sensor", "Measure", TS1, TS2, TS3) %>%
  ggplot(aes(x=Date, y=Measure, group=Sensor, color=Sensor)) +
  geom_line()

dataRaw %>%
  gather("Sensor", "Measure", TS1, TS2, TS3) %>%
  ggplot(aes(x=Date, y=Measure, group=Sensor, color=Sensor)) +
  geom_smooth()

dataRaw %>%
  gather("Sensor", "Measure", TS1, TS2, TS3) %>%
  ggplot(aes(x=Date, y=Measure, group=Sensor, color=Sensor)) +
  geom_boxplot()
```

We observe three different distributions for each sensor. Visually they appear to be some linear variation, in fact, a simple additive relationship that could be modelled by adding a scalar intercept value. This **dependency** relationship can be modelled in a BN, but, what is the best way to do it?

## Latent variables

The solution is elegant from the computational and the statistical point of view. We can think that the sensors are **measuring** some real and not observable temperature from the environment. This hidden value conditions the exact measures that the sensors capture, and thus can model its individual distributions.

In other words, our model requires an additional variable, more concretelly a **Temp** latent variable that cannot be observed but that is captured by each sensor by following a particular dependency, we will represent this by adding a new variable to our data (we will also remove unused variables).

```{r}
dataLatent <- dataRaw %>% 
  mutate(Temp = NA) %>%
  select(Temp, TS1, TS2, TS3) %>%
  as.data.frame()

dataLatent$Temp <- as.numeric(dataLatent$Temp)
```

The next step is to estimate this variable to complete our model. We can learn the three network distributions from the data but not the Temp variable. For that we would use the `EM` algorithm. First, lets build the model:

```{r}
heatAlarmDag1 <- model2network("[Temp][TS1|Temp][TS2|Temp][TS3|Temp]")
plotD3bn(heatAlarmDag1)
```


Now for the EM, unfortunatelly `bnlearn` does not provide a standalone implementation of this algorithm (it does for structural learning EM which is a superset), but it provides the tools to to so. Basically, the parametric EM algorithm can be implemented by:

* **E step:** Impute the missing variables in the data by using Bayesian estimation from the actual model.
* **M step:** Learning new parameters for the model by maximizing the likelihood from the imputten data (MLE).

As you can imagine these steps would be repetead though a number of iterations or convergence. To impute the latent variables we will use the function `inpute`, **however**, we would need an initial model to do so. This is another important element for the EM algorithm, as we would need a good guess to ensure a proper fit.

Lets first try with a random initialization of Temp

```{r}
# Lets add uniform Temp 
dataImputed <- dataLatent %>% rowwise() %>% mutate(Temp = runif(1, 10, 25))

dataImputed %>%
  ggplot(aes(x=Temp)) + geom_density()
```

The EM algorithm is then plain simple

```{r}
parametric.em <- function(dag, dataLatent, dataImputed, iter = 5) {
  fitted <- bn.fit(dag, dataImputed, method = "mle")
  for ( i in seq(iter)) {
   complete <- impute(fitted, data = dataLatent, method = "bayes-lw")
   fitted <- bn.fit(dag, complete, method = "mle")
  }
  fitted
}
```

Let's try it

```{r}
heatAlarmModel1 <- parametric.em(heatAlarmDag1, dataLatent, dataImputed, iter = 10)
```

Lets see the resulting model:

```{r}
heatAlarmModel1

# Sample some data and plot it
impute(heatAlarmModel1, data = dataLatent, method = "bayes-lw") %>%
gather("Sensor", "Measure") %>%
ggplot(aes(x=Measure, color=Sensor)) + geom_density()
```

Look at those intercept coefficient, they do not look right. As we can see in the plot it seems like a forced fit, where the Temp variable has a very different distribution. Lets try another guess at the prior for Temp, this time we will use a Gaussian distribution with the ststistics drawn from the sensors.


```{r}

statistics <- dataLatent %>% 
  gather("Sensor", "Measure", TS1, TS2, TS3) %>%
  summarise( 
    mu    = mean(Measure),
    sigma = sd(Measure)
  )

dataImputed <- dataLatent %>% 
  rowwise() %>% 
  mutate(Temp = rnorm(1, statistics$mu, statistics$sigma))

dataImputed %>%
  ggplot(aes(x=Temp)) + geom_density()
```

Let's repeat the EM:

```{r}
heatAlarmModel1 <- parametric.em(heatAlarmDag1, dataLatent, dataImputed, iter = 10)
```

```{r}
heatAlarmModel1

# Sample some data and plot it
impute(heatAlarmModel1, data = dataLatent, method = "bayes-lw") %>%
gather("Sensor", "Measure") %>%
ggplot(aes(x=Measure, color=Sensor)) + geom_density()
```

This time the fit is even worse, as we overfitted a bad prior. We will try to make a better guess, this time depedant on the actual value of the sensors.



```{r}
dataImputed <- dataLatent %>% 
  rowwise() %>% 
  mutate(Temp = mean(c(TS1, TS2, TS3)))

dataImputed %>%
  ggplot(aes(x=Temp)) + geom_density()
```

```{r}
heatAlarmModel1 <- parametric.em(heatAlarmDag1, dataLatent, dataImputed, iter = 10)
```

```{r}
heatAlarmModel1

# Sample some data and plot it
impute(heatAlarmModel1, data = dataLatent, method = "bayes-lw") %>%
gather("Sensor", "Measure") %>%
ggplot(aes(x=Measure, color=Sensor)) + geom_density()
```

Now it seems that we got a nice fit, the density graph represents the sensors as scalar transformations of the latent Temp variable.


## Modeling the domain

If we go back to the plot showing the evolution of the temperature along the year we can see that there could be different distributions for different periods of time. If we can obtain this discriminative information we could do better modeling the domain.

We will start by discriminating the distribution by month:

```{r}
dataByMonth <- dataRaw %>%
  mutate(Month = as.factor(month(Date, label = TRUE, abbr = TRUE)))

dataByMonth %>%
  gather("Sensor", "Measure", TS1, TS2, TS3) %>%
  ggplot(aes(x=Measure, color=Sensor, fill=Sensor)) + 
    geom_density(alpha=0.5) +
    facet_wrap(~Month)
```

It really seems that the month can capture additional information about the distribution. In this case, we would obtain a better fit to the data if we condition the Month as a discrete parent for the Temp variable. In this case we would need parameters for 12 different gaussian distributions, which may be a bit overkill.

Let's try if we could summarise the month information by creating a lower dimensional variable. If we think about the weather domain we rapidly come up with the year's seasons. Let's try to discriminate the distributions by season.

```{r}
# Lubridate cant extract seasons so we need a custom function

# We better encode the states as a dict to avoid magic strings
SeasonStates <- list(
  Winter = "winter",
  Spring = "spring",
  Summer = "summer",
  Fall = "fall"
)

season <- function(date){
  WS <- as.Date("2012-12-15", format = "%Y-%m-%d") # Winter Solstice
  SE <- as.Date("2012-3-15",  format = "%Y-%m-%d") # Spring Equinox
  SS <- as.Date("2012-6-15",  format = "%Y-%m-%d") # Summer Solstice
  FE <- as.Date("2012-9-15",  format = "%Y-%m-%d") # Fall Equinox
  
  # Convert dates from any year to 2012 dates
  d <- as.Date(strftime(date, format="2012-%m-%d"))
  
  factor(
    ifelse (d >= WS | d < SE, SeasonStates$Winter,
      ifelse (d >= SE & d < SS, SeasonStates$Spring,
        ifelse (d >= SS & d < FE, SeasonStates$Summer, SeasonStates$Fall))),
    levels = SeasonStates
  )
}

dataBySeason <- dataByMonth %>%
  mutate(Season = season(Date))

dataBySeason %>%
  gather("Sensor", "Measure", TS1, TS2, TS3) %>%
  ggplot(aes(x=Measure, color=Sensor, fill=Sensor)) + 
    geom_density(alpha=0.5) +
    facet_wrap(~Season)
```

The distributions for the season discrimination seems less Gaussian, but they encode the same pattern. Lets try to model both networks and see how well they fit the parameters.

First we will model the Month based one :

```{r}
dataLatentMonth <- dataByMonth %>%
  mutate(Temp = NA) %>%
  select(Month, Temp, TS1, TS2, TS3) %>%
  as.data.frame()

dataLatentMonth$Temp <- as.numeric(dataLatentMonth$Temp)
```

And now we model the network DAG

```{r}
heatAlarmDagMonth <- model2network("[Month][Temp|Month][TS1|Temp][TS2|Temp][TS3|Temp]")
plotD3bn(heatAlarmDagMonth)
```

Lets repeat the EM example and induce the parameters of the hidden variable with the previous successful configuration.

```{r}
dataImputedMonth <- dataLatentMonth %>% 
  rowwise() %>% 
  mutate(Temp = mean(c(TS1, TS2, TS3))) %>%
  as.data.frame()
```

```{r}
heatAlarmModelMonth <- parametric.em(heatAlarmDagMonth, dataLatentMonth, dataImputedMonth, iter = 10)
```

```{r}
dataImputeMonthPosterior <- impute(heatAlarmModelMonth, data = dataLatentMonth, method = "bayes-lw")

dataImputeMonthPosterior %>%
  gather("Sensor", "Measure", Temp, TS1, TS2, TS3) %>%
  ggplot(aes(x=Measure, color=Sensor, fill=Sensor)) + 
    geom_density(alpha=0.5) +
    facet_wrap(~Month)
```

This is a very good fit indeed. We can see that Temp is almost identical to TS3, and that the other two sensors are just transformations of this variable. Lets check it the season is a good simplification of the Month variable.

```{r}
dataLatentSeason <- dataBySeason %>%
  mutate(Temp = NA) %>%
  select(Season, Temp, TS1, TS2, TS3) %>%
  as.data.frame()

dataLatentSeason$Temp <- as.numeric(dataLatentSeason$Temp)

heatAlarmDagSeason <- model2network("[Season][Temp|Season][TS1|Temp][TS2|Temp][TS3|Temp]")

dataImputedSeason <- dataLatentSeason %>% 
  rowwise() %>% 
  mutate(Temp = mean(c(TS1, TS2, TS3))) %>%
  as.data.frame()
```

With that configuration we run the EM

```{r}
heatAlarmModelSeason <- parametric.em(heatAlarmDagSeason, dataLatentSeason, dataImputedSeason, iter = 10)
```

```{r}
dataImputeSeasonPosterior <- impute(heatAlarmModelSeason, data = dataLatentSeason, method = "bayes-lw")

dataImputeSeasonPosterior %>%
  gather("Sensor", "Measure", Temp, TS1, TS2, TS3) %>%
  ggplot(aes(x=Measure, color=Sensor, fill=Sensor)) + 
    geom_density(alpha=0.5) +
    facet_wrap(~Season)
```

Another good fit. Which one of the two approaches is better?. The initial guess should be the Season one, as it has fewer parameters than using the Month. Let's ask the BIC score.

```{r}
print(BIC(heatAlarmModelSeason, dataImputeSeasonPosterior))
print(BIC(heatAlarmModelMonth, dataImputeMonthPosterior))
```
As expected, season is by far more superior at minimizing BIC than month and should be our pick.


## Modeling anomalies

One of the most appealing aspects of Bayesian Networks is the ability to include expert knowledge in the model. While this is almost impossible in other frameworks it is a natural concepts in this case.

We call BNs open box models because we can understand and interpretate the parameters. This is really useful for modelling anomalies, because we can use latent variables and expert knowledge to represent events that cannot be observed.

If the events cannot be observed we would not have available data to learn from. In our example we would have to destroy machines and lit the room on fire to capture data about anomalies, and that would surely have a high cost. Imagine other scenarios such as rare diseases, autonomous driving or financial fraud.

We will start by modeling a global fault by adding a new variable `Alarm`, this variable will have two states `yes` and `no`. In this example we have been collecting data from the sensors for a year, we assume that the functioning has been correct and that there has not been recorder failures (perhaps because the expert technicians that maintain the room have assured us so).

Given this, the Alarm variable will be always observed to `no`.

```{r}
# New Variables
AlarmStates <- list(
  No  = "no",
  Yes = "yes"
)

dataLatent <- dataLatentSeason %>%
  mutate(Alarm = factor(AlarmStates$No, levels = AlarmStates)) %>%
  select(Alarm, Season, Temp, TS1, TS2, TS3) %>%
  as.data.frame()
```

And now the new dag:

```{r}
heatAlarmDag <- model2network("[Alarm][Season][Temp|Season:Alarm][TS1|Temp][TS2|Temp][TS3|Temp]")
plotD3bn(heatAlarmDag)
```

Lets run the EM algorithm one more time, ther should be no changes to the parameters but it will reinforce our logical pipeline.

```{r}
dataImputed <- dataLatent %>% 
  rowwise() %>% 
  mutate(Temp = mean(c(TS1, TS2, TS3))) %>%
  as.data.frame()
```

```{r}
heatAlarmModel <- parametric.em(heatAlarmDag, dataLatent, dataImputed, iter = 10)
```

Lets check the result on the Alarm variable:

```{r}
heatAlarmModel$Alarm
```

Obviously the algorithm will fit all the cases to the observed population in which no alarms have occurred. Here is where the **expert knowledge** enters. We will contact our experts and design a proper parametrization of the variable.

> "The system has a random failure rate of 1/1000, in such cases, the room temperature is quickly raised about 10".

We have enough information to include this expert knowledge into de Alarm variable. Notice that this variable introduces additional parameters into the Temp variable that should be modelled to.

```{r}
cptAlarm <- coef(heatAlarmModel$Alarm)
print(cptAlarm)
```

```{r}
cptAlarm[] <- c(0.999, 0.001)
heatAlarmModel$Alarm <- cptAlarm
print(heatAlarmModel$Alarm)
```

Now for the Temp variable, which a little more complex

```{r}
cgaussTemp <- coef(heatAlarmModel$Temp)
sdTemp     <- heatAlarmModel$Temp$sd
print(cgaussTemp)
print(sdTemp)
```

```{r}
cgaussTemp[is.nan(cgaussTemp)] <- cgaussTemp[!is.nan(cgaussTemp)] + 10
sdTemp[is.nan(sdTemp)] <- sdTemp[!is.nan(sdTemp)]

heatAlarmModel$Temp <- list( coef = cgaussTemp, sd = sdTemp)
heatAlarmModel$Temp
```

We have a complete model now, with a modelled anomaly detection schema. Lets test some queries.


```{r}
e <- list( "Season" = SeasonStates$Winter, "TS1" = 23, "TS2" = 33, "TS3" = 29 )
query <- cpquery(heatAlarmModel, event = Alarm == "yes", evidence = e, method = "lw", n = 100000)
print(mean(rep(query, 10000)))
```

This temperature is too high for winter, so the alarm is likely to go up.

```{r}
e <- list( "Season" = SeasonStates$Summer, "TS1" = 23, "TS2" = 33, "TS3" = 29 )
query <- cpquery(heatAlarmModel, event = Alarm == "yes", evidence = e, method = "lw", n = 100000)
print(mean(rep(query, 10000)))
```

However this is a good fit for the higher temperatures that are registered in summer.


## More anomalies

We could model additional anomalies for the individual malfunctioning of each sensor. In that case we could observe either global temperature anomalies or individual malfunctions.

A common pattern is to add a new hidden discrete variable conditioning each sensor with a random distribution in the case of malfunction.

```{r}
# New Variables
TS1FaultStates <- list(
  No  = "no",
  Yes = "yes"
)

dataLatent <- dataLatentSeason %>%
  mutate(
    Alarm = factor(AlarmStates$No, levels = AlarmStates),
    TS1Fault = factor(TS1FaultStates$No, levels = TS1FaultStates)
  ) %>%
  select(Alarm, Season, Temp, TS1Fault, TS1, TS2, TS3) %>%
  as.data.frame()

dataImputed <- dataLatent %>% 
  rowwise() %>% 
  mutate(Temp = mean(c(TS1, TS2, TS3))) %>%
  as.data.frame()

heatAlarmDag <- model2network("[Alarm][Season][TS1Fault][Temp|Season:Alarm][TS1|Temp:TS1Fault][TS2|Temp][TS3|Temp]")
plotD3bn(heatAlarmDag)
```

We launch EM and modify the new probability of the nodes (now Alarm and TS1 Fault). For that we need additional exprt knowledge from the domain. We ask our IoT team about the fault tolerance of our sensors. They tell us the following>

> Our sensors are the best, they usually do not fail. The testing guys estimate just a 1/1000 failures.

This is good, with this marginal error probbility out new node will only detect anomalies without introducing noise in the remaining propagation within the model.

```{r}
heatAlarmModel <- parametric.em(heatAlarmDag, dataLatent, dataImputed, iter = 10)
```

```{r}
# TODO: Please dont do this, this is not DRY...
cptAlarm[] <- c(0.999, 0.001)
heatAlarmModel$Alarm <- cptAlarm
cgaussTemp <- coef(heatAlarmModel$Temp)
sdTemp     <- heatAlarmModel$Temp$sd
cgaussTemp[is.nan(cgaussTemp)] <- cgaussTemp[!is.nan(cgaussTemp)] + 10
sdTemp[is.nan(sdTemp)] <- sdTemp[!is.nan(sdTemp)]
heatAlarmModel$Temp <- list( coef = cgaussTemp, sd = sdTemp)
```


```{r}
cptTS1Fault <- coef(heatAlarmModel$TS1Fault)
print(cptTS1Fault)
```

```{r}
cptTS1Fault[] <- c(0.9999, 0.0001)
heatAlarmModel$TS1Fault <- cptTS1Fault
print(heatAlarmModel$TS1Fault)
```

And now for the TS1 node, in which we have to add a random distribution. We can use a Gaussian with lots of variance.

```{r}
cgaussTS1 <- coef(heatAlarmModel$TS1)
sdTS1     <- heatAlarmModel$TS1$sd
print(cgaussTS1)
print(sdTS1)
```

```{r}
cgaussTS1[is.nan(cgaussTS1)] <- 0
sdTS1[is.nan(sdTS1)] <- 1000000

heatAlarmModel$TS1 <- list( coef = cgaussTS1, sd = sdTS1)
heatAlarmModel$TS1
```

Lets test some inference cases in which the TS1 sensor is malfunctioning or reporting some kind of anomalous measures.

If TS1 is only one capturing a very high temperature the fault node will detect the anomaly

```{r}
e <- list( "Season" = SeasonStates$Winter, "TS1" = 35, "TS2" = 24, "TS3" = 20 )
query <- cpquery(heatAlarmModel, event = Alarm == "yes", evidence = e, method = "lw", n = 100000)
print(mean(rep(query, 10000)))
```

```{r}
e <- list( "Season" = SeasonStates$Winter, "TS1" = 35, "TS2" = 24, "TS3" = 20 )
query <- cpquery(heatAlarmModel, event = TS1Fault == "yes", evidence = e, method = "lw", n = 100000)
print(mean(rep(query, 10000)))
```

If the three sensors are capturing the same anomaly, then the TS1Fault node will assume that the problem is outside the model.

```{r}
e <- list( "Season" = SeasonStates$Winter, "TS1" = 35, "TS2" = 37, "TS3" = 29 )
query <- cpquery(heatAlarmModel, event = Alarm == "yes", evidence = e, method = "lw", n = 100000)
print(mean(rep(query, 10000)))
```

```{r}
e <- list( "Season" = SeasonStates$Winter, "TS1" = 35, "TS2" = 37, "TS3" = 29 )
query <- cpquery(heatAlarmModel, event = TS1Fault == "yes", evidence = e, method = "lw", n = 100000)
print(mean(rep(query, 10000)))
```



## A more complex problem

This is a variation on the previous problem. Can you spot the differences introduced by this new version?

```{r}
dataRaw <- read_csv("./data/heatAlarm-lvl2.csv")
```

It seems that the measures are somehow dependent, but the relationship is not that clear that in the previous phase. An ex

```{r}
dataRaw %>%
  gather("Sensor", "Measure", TS1, TS2, TS3) %>%
  ggplot(aes(x=Date, y=Measure, group=Sensor, color=Sensor)) +
  geom_line()

dataRaw %>%
  gather("Sensor", "Measure", TS1, TS2, TS3) %>%
  ggplot(aes(x=Date, y=Measure, group=Sensor, color=Sensor)) +
  geom_smooth()
```

### Looking at the context

> Our experts tell us that the indoor temperature of the room is affected by the outdoor weather, specially near the windows and doors. The temperature in the interior part of the room is more stable.

Given this new knowledge we decide to place a new sensor outdoors `TSO`. Now we can have an additional component to add to our model, hopefully it will allow to mixture the indoor and outdoor temperatures.

```{r}
dataRaw %>%
  gather("Sensor", "Measure", TSO, TS1, TS2, TS3) %>%
  ggplot(aes(x=Date, y=Measure, group=Sensor, color=Sensor)) +
  geom_smooth()
```

```{r}
dataByMonth <- dataRaw %>%
  mutate(Month = as.factor(month(Date, label = TRUE, abbr = TRUE)))

dataByMonth %>%
  gather("Sensor", "Measure", TSO, TS1, TS2, TS3) %>%
  ggplot(aes(x=Measure, color=Sensor, fill=Sensor)) + 
    geom_density(alpha=0.5) +
    facet_wrap(~Month)
```

```{r}
dataBySeason <- dataByMonth %>%
  mutate(Season = season(Date))

dataBySeason %>%
  gather("Sensor", "Measure", TSO, TS1, TS2, TS3) %>%
  ggplot(aes(x=Measure, color=Sensor, fill=Sensor)) + 
    geom_density(alpha=0.4) +
    facet_wrap(~Season)
```

### Adding new components to the model

To add the new sensor to the model we are going to need an additional latent variable. Our model will now have two latent variables to model the temperature `TInd` and `TOut` (indoor and outdoor). The Indoor temperature will depend on the outdoor temperature. Lets build it:

```{r}
heatAlarmDag2 <- model2network("[Alarm][Season][TOut|Season][TS0|TOut][TInd|TOut:Alarm][TS1|TInd][TS2|TInd][TS3|TInd]")

plotD3bn(heatAlarmDag2)
```

This model is the base example of the new domain. However we can have some doubts regarding the relationship of the variable TOut and the models, look closely at the distributions and you will see that not all distributions are affected equally by the outdoor temperature. We ask again...

> IoT team: Uh, yes. We instaled TS1 in a corner of the room, near the entrance. TS2 is on the ceiling and might be near a ventilation shaft, so it will probably capture a bit more of heat from the machines. TS3 should be more stable, it is located in the middle of the room.

This almost make sense visually. However, at this point we will need to test the different structures, either by learning all of them and comparing with BIC, or running a structural learning process.

---

---

# License

This tutorial is licensed under [GPL3 license](https://www.gnu.org/licenses/lgpl-3.0.en.html) as is originally published and maintained in this [github repository](https://github.com/jacintoArias/bayesnetRtutorial) by [Jacinto Arias](http://jarias.es)
